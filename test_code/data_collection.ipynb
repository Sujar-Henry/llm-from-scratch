{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e46413",
   "metadata": {},
   "source": [
    "# Data Collection for LLM Training\n",
    "\n",
    "This notebook demonstrates how to collect and preprocess text data for training a language model. We'll use \"The Adventures of Sherlock Holmes\" from Project Gutenberg as our example dataset.\n",
    "\n",
    "## Steps covered:\n",
    "1. **Download** text data from Project Gutenberg\n",
    "2. **Load and inspect** the raw text\n",
    "3. **Tokenize** the text by splitting on punctuation and spaces\n",
    "4. **Clean** the tokens by removing empty strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaa5a9b",
   "metadata": {},
   "source": [
    "## 1. Download Text Data\n",
    "\n",
    "We'll download \"The Adventures of Sherlock Holmes\" from Project Gutenberg, which provides free access to thousands of books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c20abfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ File already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Check if we already have the file to avoid re-downloading\n",
    "if not os.path.exists(\"sherlock-holmes.txt\"):\n",
    "    # URL for \"The Adventures of Sherlock Holmes\" from Project Gutenberg\n",
    "    url = (\"https://www.gutenberg.org/files/1661/1661-0.txt\")\n",
    "    file_path = \"sherlock-holmes.txt\"\n",
    "    \n",
    "    # Download the file and save it locally\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "    print(\"âœ… Downloaded Sherlock Holmes text\")\n",
    "else:\n",
    "    print(\"ðŸ“„ File already exists, skipping download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1aa1149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 581425\n",
      "\n",
      "ðŸ“– First 1000 characters:\n",
      "--------------------------------------------------\n",
      "ï»¿The Project Gutenberg eBook of The Adventures of Sherlock Holmes,\n",
      "by Arthur Conan Doyle\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org. If you are not located in the United States, you\n",
      "will have to check the laws of the country where you are located before\n",
      "using this eBook.\n",
      "\n",
      "Title: The Adventures of Sherlock Holmes\n",
      "\n",
      "Author: Arthur Conan Doyle\n",
      "\n",
      "Release Date: November 29, 2002 [eBook #1661]\n",
      "[Most recently updated: October 10, 2023]\n",
      "\n",
      "Language: English\n",
      "\n",
      "Character set encoding: UTF-8\n",
      "\n",
      "Produced by: an anonymous Project Gutenberg volunteer and Jose Menendez\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK THE ADVENTURES OF SHERLOCK\n",
      "HOLMES ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Adventures of Sherlock Holmes\n",
      "\n",
      "by Arthur Conan Doyle\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "   I.     A Scandal in Bohemia\n",
      "   II.    The Red-Headed League\n",
      "   III.   A Case of Identity\n",
      "   IV.    The Boscombe Valley Mystery\n",
      "   V.     The Five Orange Pips\n",
      "   VI.    The Man with the Twisted Lip\n",
      "   VII.   The Adventure of the Blue Carbuncle\n",
      "   VIII.  The Adventure of the Speckled Band\n",
      "   IX.    The Adventure of the Engineerâ€™s Thumb\n",
      "   X.     The Adventure of the Noble Bachelor\n",
      "   XI.    The Adventure of the Beryl Coronet\n",
      "   XII.   The Adventure of the Copper Beeches\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I. A SCANDAL IN BOHEMIA\n",
      "\n",
      "\n",
      "I.\n",
      "\n",
      "To Sherlock Holmes she is always _the_ woman. I have seldom heard him\n",
      "mention her under any other name. In his eyes she eclipses and\n",
      "predominates the whole of her sex. It was not that he felt any emotion\n",
      "akin to love for Irene Adler. All emotions, and that one particularly,\n",
      "were abhorrent to his cold, precise but admirably balanced mind. He\n",
      "was, I take it, the most perfect reasoning and observing machine that\n",
      "the world has seen, but as a lover he would have placed himself in a\n",
      "false position. He never spoke of the softer passions, save with a gibe\n",
      "and a sneer. They were admirable things for the observerâ€”excellent for\n",
      "drawing the veil from menâ€™s motives and actions. But for the trained\n",
      "reasoner to admit such intrusions into his own delicate and finely\n",
      "adjusted temperament was to introduce a distracting factor which might\n",
      "throw a doubt upon all his mental results. Grit in a sensitive\n",
      "instrument, or a crack in one of his own high-power lenses, would not\n",
      "be more disturbing than a strong emotion in a nature such as his. And\n",
      "yet there was but one woman to him, and that woman was the late Irene\n",
      "Adler, of dubious and questionable memory.\n",
      "\n",
      "I had seen little of Holmes lately. My marriage had drifted us away\n",
      "from each other. My own complete happiness, and the home-centred\n",
      "interests which rise up around the man who first finds himself master\n",
      "of his own establishment, were sufficient to absorb all my attention,\n",
      "while Holmes, who loathed every form of society with his whole Bohemian\n",
      "soul, remained in our lodgings in Baker Street, buried among his old\n",
      "books, and alternating from week to week between cocaine and ambition,\n",
      "the drowsiness of the drug, and the fierce energy of his own keen\n",
      "nature. He was still, as ever, deeply attracted by the study of crime,\n",
      "and occupied his immense faculties and extraordinary powers of\n",
      "observation in following out those clues, and clearing up those\n",
      "mysteries which had been abandoned as hopeless by the official police.\n",
      "From time to time I heard some vague account of his doings: of his\n",
      "summons to Odessa in the case of the Trepoff murder, of his clearing up\n",
      "of the singular tragedy of the Atkinson brothers at Trincomalee, and\n",
      "finally of the mission which he had accomplished so delicately and\n",
      "successfully for the reigning family of Holland. Beyond these signs of\n",
      "his activity, however, which I merely shared with all the readers of\n",
      "the daily press, I knew little of my former friend and companion.\n",
      "\n",
      "One nightâ€”it was on the twentieth of March, 1888â€”I was returning from a\n",
      "journey to a patient (for I had now returned to civil practice), when\n",
      "my way led me through Baker Street. As I passed the well-remembered\n",
      "door, which must always be associated in my mind with my wooing, and\n",
      "with the dark incidents of the Study in Scarlet, I was seized with a\n",
      "keen desire to see Holmes again, and to know how he was employing his\n",
      "extraordinary powers. His rooms were brilliantly lit, and, even as I\n",
      "looked up, I saw his tall, spare figure pass twice in a dark silhouette\n",
      "against the blind. He was pacing the room swiftly, eagerly, with his\n",
      "head sunk upon his chest and his hands clasped behind him. To me, who\n",
      "knew his every mood and habit, his attitude and manner told their own\n",
      "story. He was at work again. He had risen out of his drug-created\n",
      "dreams and was hot upon the scent of some new problem. I rang the bell\n",
      "and was shown up to the chamber which had formerly been in part my own.\n",
      "\n",
      "His manner was not effusive. It seldom was; but he was glad, I think,\n",
      "to see me. With hardly \n"
     ]
    }
   ],
   "source": [
    "# Load the entire text file into memory\n",
    "with open(\"sherlock-holmes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# Check the size and preview the content\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(\"\\nðŸ“– First 1000 characters:\")\n",
    "print(\"-\" * 50)\n",
    "print(raw_text[:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc11a6d",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect the Text\n",
    "\n",
    "Now let's load the downloaded text file and see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba136e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting on whitespace (keeping delimiters):\n",
      "['Test', ' ', 'for', ' ', 'Sherlock', ' ', 'Holmes.', ' ', '!', ' ', '?', ' ', '/']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Example 1: Simple split on whitespace\n",
    "text = \"Test for Sherlock Holmes. ! ? /\"\n",
    "\n",
    "# re.split with (\\s) captures the delimiter (whitespace) in the result\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(\"Splitting on whitespace (keeping delimiters):\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e50c00",
   "metadata": {},
   "source": [
    "## 3. Text Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into smaller units (tokens) that can be processed by our language model. Let's start with some simple examples to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f33405f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting on punctuation and whitespace:\n",
      "['Test', ' ', 'for', ' ', 'Sherlock', ' ', 'Holmes', '.', '', ' ', '!', ' ', '?', ' ', '/']\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Split on punctuation AND whitespace\n",
    "# This captures commas, periods, and spaces as separate tokens\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(\"Splitting on punctuation and whitespace:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5331564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Test', 'for', 'Sherlock', 'Holmes', '.', '!', '?', '/']\n"
     ]
    }
   ],
   "source": [
    "# Strip whitespace from each item and then filter out any empty strings.\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "298103d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63b39ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'Adventures', 'of', 'Sherlock', 'Holmes', ',', 'by', 'Arthur', 'Conan', 'Doyle', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea38ee0",
   "metadata": {},
   "source": [
    "## 4. Apply Tokenization to Full Text\n",
    "\n",
    "Now let's apply our tokenization strategy to the entire Sherlock Holmes text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3675c8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126189\n",
      "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'Adventures', 'of', 'Sherlock', 'Holmes', ',', 'by', 'Arthur', 'Conan', 'Doyle', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', 'You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www', '.', 'gutenberg', '.', 'org', '.', 'If', 'you', 'are', 'not', 'located', 'in', 'the', 'United', 'States', ',', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'eBook', '.', 'Title', ':', 'The', 'Adventures', 'of', 'Sherlock', 'Holmes', 'Author', ':', 'Arthur', 'Conan', 'Doyle', 'Release', 'Date', ':', 'November', '29', ',', '2002', '[eBook', '#1661]', '[Most', 'recently', 'updated', ':', 'October', '10', ',', '2023]', 'Language', ':', 'English', 'Character', 'set', 'encoding', ':', 'UTF-8', 'Produced', 'by', ':', 'an', 'anonymous', 'Project', 'Gutenberg', 'volunteer', 'and', 'Jose', 'Menendez', '***', 'START', 'OF', 'THE', 'PROJECT', 'GUTENBERG', 'EBOOK', 'THE', 'ADVENTURES', 'OF', 'SHERLOCK', 'HOLMES', '***', 'The', 'Adventures', 'of', 'Sherlock', 'Holmes', 'by', 'Arthur', 'Conan', 'Doyle', 'Contents', 'I', '.', 'A', 'Scandal', 'in', 'Bohemia', 'II', '.', 'The', 'Red-Headed', 'League', 'III', '.', 'A', 'Case', 'of', 'Identity', 'IV', '.', 'The', 'Boscombe', 'Valley', 'Mystery', 'V', '.', 'The', 'Five', 'Orange', 'Pips', 'VI', '.', 'The', 'Man', 'with', 'the', 'Twisted', 'Lip', 'VII', '.', 'The', 'Adventure', 'of', 'the', 'Blue', 'Carbuncle', 'VIII', '.', 'The', 'Adventure', 'of', 'the', 'Speckled', 'Band', 'IX', '.', 'The', 'Adventure', 'of', 'the', 'Engineerâ€™s', 'Thumb', 'X', '.', 'The', 'Adventure', 'of', 'the', 'Noble', 'Bachelor', 'XI', '.', 'The', 'Adventure', 'of', 'the', 'Beryl', 'Coronet', 'XII', '.', 'The', 'Adventure', 'of', 'the', 'Copper', 'Beeches', 'I', '.', 'A', 'SCANDAL', 'IN', 'BOHEMIA', 'I', '.', 'To', 'Sherlock', 'Holmes', 'she', 'is', 'always', '_', 'the', '_', 'woman', '.', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex', '.', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', '.', 'All', 'emotions', ',', 'and', 'that', 'one', 'particularly', ',', 'were', 'abhorrent', 'to', 'his', 'cold', ',', 'precise', 'but', 'admirably', 'balanced', 'mind', '.', 'He', 'was', ',', 'I', 'take', 'it', ',', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen', ',', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position', '.', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions', ',', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer', '.', 'They', 'were', 'admirable', 'things', 'for', 'the', 'observerâ€”excellent', 'for', 'drawing', 'the', 'veil', 'from', 'menâ€™s', 'motives', 'and', 'actions', '.', 'But', 'for', 'the', 'trained', 'reasoner', 'to', 'admit', 'such', 'intrusions', 'into', 'his', 'own', 'delicate', 'and', 'finely', 'adjusted', 'temperament', 'was', 'to', 'introduce', 'a', 'distracting', 'factor', 'which', 'might', 'throw', 'a', 'doubt', 'upon', 'all', 'his', 'mental', 'results', '.', 'Grit', 'in', 'a', 'sensitive', 'instrument', ',', 'or', 'a', 'crack', 'in', 'one', 'of', 'his', 'own', 'high-power', 'lenses', ',', 'would', 'not', 'be', 'more', 'disturbing', 'than', 'a', 'strong', 'emotion', 'in', 'a', 'nature', 'such', 'as', 'his', '.', 'And', 'yet', 'there', 'was', 'but', 'one', 'woman', 'to', 'him', ',', 'and', 'that', 'woman', 'was', 'the', 'late', 'Irene', 'Adler', ',', 'of', 'dubious', 'and', 'questionable', 'memory', '.', 'I', 'had', 'seen', 'little', 'of', 'Holmes', 'lately', '.', 'My', 'marriage', 'had', 'drifted', 'us', 'away', 'from', 'each', 'other', '.', 'My', 'own', 'complete', 'happiness', ',', 'and', 'the', 'home-centred', 'interests', 'which', 'rise', 'up', 'around', 'the', 'man', 'who', 'first', 'finds', 'himself', 'master', 'of', 'his', 'own', 'establishment', ',', 'were', 'sufficient', 'to', 'absorb', 'all', 'my', 'attention', ',', 'while', 'Holmes', ',', 'who', 'loathed', 'every', 'form', 'of', 'society', 'with', 'his', 'whole', 'Bohemian', 'soul', ',', 'remained', 'in', 'our', 'lodgings', 'in', 'Baker', 'Street', ',', 'buried', 'among', 'his', 'old', 'books', ',', 'and', 'alternating', 'from', 'week', 'to', 'week', 'between', 'cocaine', 'and', 'ambition', ',', 'the', 'drowsiness', 'of', 'the', 'drug', ',', 'and', 'the', 'fierce', 'energy', 'of', 'his', 'own', 'keen', 'nature', '.', 'He', 'was', 'still', ',', 'as', 'ever', ',', 'deeply', 'attracted', 'by', 'the', 'study', 'of', 'crime', ',', 'and', 'occupied', 'his', 'immense', 'faculties', 'and', 'extraordinary', 'powers', 'of', 'observation', 'in', 'following', 'out', 'those', 'clues', ',', 'and', 'clearing', 'up', 'those', 'mysteries', 'which', 'had', 'been', 'abandoned', 'as', 'hopeless', 'by', 'the', 'official', 'police', '.', 'From', 'time', 'to', 'time', 'I', 'heard', 'some', 'vague', 'account', 'of', 'his', 'doings', ':', 'of', 'his', 'summons', 'to', 'Odessa', 'in', 'the', 'case', 'of', 'the', 'Trepoff', 'murder', ',', 'of', 'his', 'clearing', 'up', 'of', 'the', 'singular', 'tragedy', 'of', 'the', 'Atkinson', 'brothers', 'at', 'Trincomalee', ',', 'and', 'finally', 'of', 'the', 'mission', 'which', 'he', 'had', 'accomplished', 'so', 'delicately', 'and', 'successfully', 'for', 'the', 'reigning', 'family', 'of', 'Holland', '.', 'Beyond', 'these', 'signs', 'of', 'his', 'activity', ',', 'however', ',', 'which', 'I', 'merely', 'shared', 'with', 'all', 'the', 'readers', 'of', 'the', 'daily', 'press', ',', 'I', 'knew', 'little', 'of', 'my', 'former', 'friend', 'and', 'companion', '.', 'One', 'nightâ€”it', 'was', 'on', 'the', 'twentieth', 'of', 'March', ',', '1888â€”I', 'was', 'returning', 'from', 'a', 'journey', 'to', 'a', 'patient', '(', 'for', 'I', 'had', 'now', 'returned', 'to', 'civil', 'practice', ')', ',', 'when', 'my', 'way', 'led', 'me', 'through', 'Baker', 'Street', '.', 'As', 'I', 'passed', 'the', 'well-remembered', 'door', ',', 'which', 'must', 'always', 'be', 'associated', 'in', 'my', 'mind', 'with', 'my', 'wooing', ',', 'and', 'with', 'the', 'dark', 'incidents', 'of', 'the', 'Study', 'in', 'Scarlet', ',', 'I', 'was', 'seized', 'with', 'a', 'keen', 'desire', 'to', 'see', 'Holmes', 'again', ',', 'and', 'to', 'know', 'how', 'he', 'was', 'employing', 'his', 'extraordinary', 'powers', '.', 'His', 'rooms', 'were', 'brilliantly', 'lit', ',', 'and', ',', 'even', 'as', 'I', 'looked', 'up', ',', 'I', 'saw', 'his', 'tall', ',', 'spare', 'figure', 'pass', 'twice', 'in', 'a', 'dark', 'silhouette', 'against', 'the', 'blind', '.', 'He', 'was', 'pacing', 'the', 'room', 'swiftly', ',', 'eagerly', ',', 'with', 'his', 'head', 'sunk', 'upon', 'his', 'chest', 'and', 'his', 'hands', 'clasped', 'behind', 'him', '.', 'To', 'me', ',', 'who', 'knew', 'his', 'every', 'mood', 'and', 'habit', ',', 'his', 'attitude', 'and', 'manner', 'told', 'their', 'own', 'story', '.', 'He', 'was', 'at', 'work', 'again', '.', 'He', 'had', 'risen', 'out', 'of', 'his', 'drug-created', 'dreams', 'and', 'was', 'hot', 'upon', 'the', 'scent', 'of', 'some', 'new', 'problem', '.', 'I', 'rang', 'the', 'bell', 'and', 'was', 'shown', 'up', 'to', 'the', 'chamber', 'which', 'had', 'formerly', 'been', 'in', 'part', 'my', 'own', '.', 'His', 'manner', 'was', 'not', 'effusive', '.', 'It', 'seldom', 'was', ';', 'but', 'he', 'was', 'glad', ',', 'I', 'think', ',', 'to', 'see', 'me', '.', 'With', 'hardly', 'a', 'word', 'spoken', ',', 'but', 'with', 'a', 'kindly', 'eye', ',', 'he', 'waved', 'me', 'to', 'an', 'armchair', ',', 'threw', 'across']\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))\n",
    "print(preprocessed[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a14a08",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "âœ… **What we accomplished:**\n",
    "- Downloaded a classic text from Project Gutenberg\n",
    "- Loaded and inspected the raw text data\n",
    "- Learned different tokenization approaches using regex\n",
    "- Applied comprehensive tokenization to split text into meaningful tokens\n",
    "- Cleaned the data by removing empty tokens\n",
    "\n",
    "ðŸŽ¯ **Next steps for LLM training:**\n",
    "- Create a vocabulary from these tokens\n",
    "- Convert tokens to numerical IDs\n",
    "- Organize data into training batches\n",
    "- Feed to a neural network for language model training\n",
    "\n",
    "This preprocessed token list is now ready to be used as training data for a language model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b74078",
   "metadata": {},
   "source": [
    "## Convert Tokens into Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d234e23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab_size: 9885\n"
     ]
    }
   ],
   "source": [
    "#Use a set to get rid of duplicates\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(f\"Vocab_size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f324ee9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('#1661]', 1)\n",
      "('$1', 2)\n",
      "('$5', 3)\n",
      "('&', 4)\n",
      "('&c', 5)\n",
      "('(', 6)\n",
      "(')', 7)\n",
      "('***', 8)\n",
      "(',', 9)\n",
      "('-', 10)\n",
      "('--', 11)\n",
      "('.', 12)\n",
      "('000', 13)\n",
      "('1', 14)\n",
      "('10', 15)\n",
      "('100', 16)\n",
      "('1000', 17)\n",
      "('11', 18)\n",
      "('117', 19)\n",
      "('12', 20)\n",
      "('120', 21)\n",
      "('14', 22)\n",
      "('140', 23)\n",
      "('15', 24)\n",
      "('150', 25)\n",
      "('1500', 26)\n",
      "('16A', 27)\n",
      "('17', 28)\n",
      "('1846', 29)\n",
      "('1858', 30)\n",
      "('1869', 31)\n",
      "('1870', 32)\n",
      "('1878', 33)\n",
      "('1883', 34)\n",
      "('1883â€”a', 35)\n",
      "('1884â€”there', 36)\n",
      "('1887', 37)\n",
      "('1888â€”I', 38)\n",
      "('1890', 39)\n",
      "('19th', 40)\n",
      "('2', 41)\n",
      "('20%', 42)\n",
      "('200', 43)\n",
      "('2001', 44)\n",
      "('2002', 45)\n",
      "('2023]', 46)\n",
      "('220', 47)\n",
      "('221B', 48)\n",
      "('226', 49)\n",
      "('22nd', 50)\n",
      "('25', 51)\n",
      "('250', 52)\n",
      "('26', 53)\n",
      "('27', 54)\n",
      "('270', 55)\n",
      "('29', 56)\n",
      "('2nd', 57)\n",
      "('3', 58)\n",
      "('30', 59)\n",
      "('31', 60)\n",
      "('35', 61)\n",
      "('3rd', 62)\n",
      "('4', 63)\n",
      "('40', 64)\n",
      "('4000', 65)\n",
      "('4700', 66)\n",
      "('4th', 67)\n",
      "('4Â½', 68)\n",
      "('5', 69)\n",
      "('50', 70)\n",
      "('501', 71)\n",
      "('596-1887', 72)\n",
      "('6', 73)\n",
      "('60', 74)\n",
      "('64-6221541', 75)\n",
      "('7', 76)\n",
      "('700', 77)\n",
      "('750', 78)\n",
      "('8', 79)\n",
      "('801', 80)\n",
      "('809', 81)\n",
      "('84116', 82)\n",
      "('88', 83)\n",
      "('9', 84)\n",
      "('90', 85)\n",
      "('9th', 86)\n",
      "(':', 87)\n",
      "(';', 88)\n",
      "('?', 89)\n",
      "('A', 90)\n",
      "('ACTUAL', 91)\n",
      "('ADLER', 92)\n",
      "('ADVENTURE', 93)\n",
      "('ADVENTURES', 94)\n",
      "('AGREE', 95)\n",
      "('AGREEMENT', 96)\n",
      "('AND', 97)\n",
      "('ANY', 98)\n",
      "('ANYTHING', 99)\n",
      "('ASCIIâ€', 100)\n",
      "('Abbots', 101)\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary to map tokens to integers\n",
    "vocab = {}\n",
    "\n",
    "# Iterate through all unique words and assign a unique integer ID to each token\n",
    "for integer, token in enumerate(all_words):\n",
    "    vocab[token] = integer\n",
    "\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "656f3b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6a8dd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[702, 5159, 1358, 5608, 9, 4663, 4644, 8266, 8469, 5628, 6356, 1613, 9223, 8872, 4629, 0, 702, 5159, 8872, 3255, 0]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"John is a man, he has talked to many people and works very hard!\n",
    "        John is very determined!\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f58fd09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John is a man, he has talked to many people and works very hard! John is very determined!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3304e",
   "metadata": {},
   "source": [
    "# Use GPT based Tokenizer (TikToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f37a2f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\sking\\anaconda3\\envs\\sujar-env\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\sking\\anaconda3\\envs\\sujar-env\\lib\\site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\sking\\anaconda3\\envs\\sujar-env\\lib\\site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sking\\anaconda3\\envs\\sujar-env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sking\\anaconda3\\envs\\sujar-env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sking\\anaconda3\\envs\\sujar-env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sking\\anaconda3\\envs\\sujar-env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad1aaff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiktoken version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"Tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4831df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "534b6009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13, 50256, 770, 318, 257, 1790, 20911, 329, 4856, 262, 402, 11571, 12, 17, 11241, 7509, 13, 632, 3407, 617, 21025, 2288, 11, 3146, 357, 10163, 828, 290, 2041, 3435, 588, 2488, 29953, 7225, 317, 1573, 262, 47208, 22528, 1595, 470, 423, 318, 1778, 9491, 0]\n"
     ]
    }
   ],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog.<|endoftext|> This is a short excerpt for testing the GPT-2 tokenizer. It includes some punctuation, numbers (123), and special characters like @#$%. A word the Vocabulary doesn't have is Sujar!\"\n",
    "\n",
    "integer_tokens = tokenizer.encode(text,allowed_special = {\"<|endoftext|>\"})\n",
    "print(integer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e122b90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog.<|endoftext|> This is a short excerpt for testing the GPT-2 tokenizer. It includes some punctuation, numbers (123), and special characters like @#$%. A word the Vocabulary doesn't have is Sujar!\n"
     ]
    }
   ],
   "source": [
    "string_val = tokenizer.decode(integer_tokens)\n",
    "print(string_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b32c980",
   "metadata": {},
   "source": [
    "# Data Sampling with Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5841f198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in text: 581425\n"
     ]
    }
   ],
   "source": [
    "with open(\"sherlock-holmes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "encoded_text = tokenizer.encode(raw_text)\n",
    "print(f\"Total number of characters in text: {len(raw_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "693340c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [198, 22474, 42934, 1156, 284, 465, 4692, 11, 7141, 475, 6178, 343, 1346, 12974, 2000, 13, 679, 198, 9776, 11, 314, 1011, 340, 11, 262, 749, 2818, 14607, 290, 21769, 4572, 326, 198, 1169, 995, 468, 1775, 11, 475, 355, 257, 18854, 339, 561, 423, 4624, 2241, 287, 257, 198, 9562, 2292, 13, 679, 1239, 5158, 286, 262, 32359, 30477, 11, 3613, 351, 257, 308, 32438, 198, 392, 257, 10505, 263, 13, 1119, 547, 37959, 1243, 329, 262, 22890, 960, 1069, 5666, 329, 198, 19334, 278, 262, 30615, 422, 1450, 447, 247, 82, 21508, 290, 4028, 13, 887, 329, 262, 8776, 198, 41181, 263, 284, 9159, 884, 9913, 15880, 656, 465, 898, 19217, 290, 32566, 198, 29117, 36140, 373, 284, 10400, 257, 36441, 5766, 543, 1244, 198, 16939, 257, 4719, 2402, 477, 465, 5110, 2482, 13, 402, 799, 287, 257, 8564, 198, 259, 43872, 11, 393, 257, 8469, 287, 530, 286, 465, 898, 1029, 12, 6477, 18405, 11, 561, 407, 198, 1350, 517, 14851, 621, 257, 1913, 9942, 287, 257, 3450, 884, 355, 465, 13, 843, 198, 25907, 612, 373, 475, 530, 2415, 284, 683, 11, 290, 326, 2415, 373, 262, 2739, 7181, 710, 198, 2782, 1754, 11, 286, 25292, 290, 18269, 4088, 13, 198, 198, 40, 550, 1775, 1310, 286, 17628, 16537, 13, 2011, 4845, 550, 38648, 514, 1497, 198, 6738, 1123, 584, 13, 2011, 898, 1844, 12157, 11, 290, 262, 1363, 12, 1087, 445, 198, 9446, 82, 543, 4485, 510, 1088, 262, 582, 508, 717, 7228, 2241, 4958, 198, 1659, 465, 898, 9323, 11, 547, 6751, 284, 17565, 477, 616, 3241, 11, 198, 4514, 17628, 11, 508, 2376, 35932, 790, 1296, 286, 3592, 351, 465, 2187, 45560, 666, 198, 82, 2852, 11, 6150, 287, 674, 19527, 70, 654, 287, 14372, 3530, 11, 11694, 1871, 465, 1468, 198, 12106, 11, 290, 39623, 422, 1285, 284, 1285, 1022, 15144, 290, 20505, 11, 198, 1169, 288, 8516, 1272, 286, 262, 2563, 11, 290, 262, 14800, 2568, 286, 465, 898, 13795, 198, 21353, 13, 679, 373, 991, 11, 355, 1683, 11, 7744, 12725, 416, 262, 2050, 286, 4065, 11, 198, 392, 12030, 465, 13964, 47126, 290, 11359, 5635, 286, 198, 672, 3168, 341, 287, 1708, 503, 883, 20195, 11, 290, 17304, 510, 883, 198, 1820, 1706, 444, 543, 550, 587, 9958, 355, 23292, 416, 262, 1743, 1644, 13, 198, 4863, 640, 284, 640, 314, 2982, 617, 13443, 1848, 286, 465, 466, 654, 25, 286, 465, 198, 16345, 11567, 284, 10529, 21411, 287, 262, 1339, 286, 262, 4700, 79, 2364, 5123, 11, 286, 465, 17304, 510, 198, 1659, 262, 18032, 13574, 286, 262, 49042, 9397, 379, 33822, 785, 1000, 68, 11, 290, 198, 69, 3289, 286, 262, 4365, 543, 339, 550, 13013, 523, 8675, 1286, 290, 198, 37351, 329, 262, 40098, 1641, 286, 16070, 13, 12197, 777, 5895, 286, 198, 14363, 3842, 11, 2158, 11, 543, 314, 6974, 4888, 351, 477, 262, 7183, 286, 198, 1169, 4445, 1803, 11, 314, 2993, 1310, 286, 616, 1966, 1545, 290, 15185, 13, 198, 198, 3198, 1755, 960, 270, 373, 319, 262, 29112, 286, 2805, 11, 49584, 960, 40, 373, 8024, 422, 257, 198, 73, 5604, 284, 257, 5827, 357, 1640, 314, 550, 783, 4504, 284, 3026, 3357, 828, 618, 198, 1820, 835, 2957, 502, 832, 14372, 3530, 13, 1081, 314, 3804, 262, 880, 12, 2787, 368, 9451, 198, 9424, 11, 543, 1276, 1464, 307, 3917, 287, 616, 2000, 351, 616, 36440, 278, 11, 290, 198, 4480, 262, 3223, 10207, 286, 262, 12481, 287, 26620, 11, 314, 373, 12000, 351, 257, 198, 365, 268, 6227, 284, 766, 17628, 757, 11, 290, 284, 760, 703, 339, 373, 26490, 465, 198, 26086, 35947, 5635, 13, 2399, 9519, 547, 37928, 6578, 11, 290, 11, 772, 355, 314, 198, 5460, 276, 510, 11, 314, 2497, 465, 7331, 11, 13952, 3785, 1208, 5403, 287, 257, 3223, 41834, 198, 32826, 262, 7770, 13, 679, 373, 37572, 262, 2119, 23994, 11, 30130, 11, 351, 465, 198, 2256, 24790, 2402, 465, 7721, 290, 465, 2832, 47425, 276, 2157, 683, 13, 1675, 502, 11, 508, 198, 74, 3605, 465, 790, 10038, 290, 7947, 11, 465, 9408, 290, 5642, 1297, 511, 898, 198, 13571, 13, 679, 373, 379, 670, 757, 13, 679, 550, 17450, 503, 286, 465, 2563, 12, 25598, 198, 25966, 82, 290, 373, 3024, 2402, 262, 21212, 286, 617, 649, 1917, 13, 314, 28077, 262, 8966, 198, 392, 373, 3402, 510, 284, 262, 11847, 543, 550, 15734, 587, 287, 636, 616, 898, 13, 198, 198, 6653, 5642, 373, 407, 914, 11350, 13, 632, 25129, 373, 26, 475, 339, 373, 9675, 11, 314, 892, 11, 198, 1462, 766, 502, 13, 2080, 8941, 257, 1573, 9635, 11, 475, 351, 257, 26820, 4151, 11, 339, 26834, 198, 1326, 284, 281, 3211, 16337, 11, 9617, 1973, 465, 1339, 286, 33204, 11, 290, 8203, 257, 198, 38685, 1339, 290, 257, 3623, 20878, 287, 262, 5228, 13, 3244, 339, 6204, 878, 262, 2046, 198, 392, 3114, 502, 625, 287, 465, 18032, 18951, 49540, 6977, 13, 198, 198, 447, 250, 19864, 5354, 14803, 345, 11, 447, 251, 339, 24998, 13, 564, 250, 40, 892, 11, 14959, 11, 326, 345, 423, 1234, 198, 261, 3598, 290, 257, 2063, 8059, 1201, 314, 2497, 345, 13, 447, 251, 198, 198, 447, 250, 31334, 0, 447, 251, 314, 9373, 13, 198, 198, 447, 250, 17854, 11, 314, 815, 423, 1807, 257, 1310, 517, 13, 2329, 257, 491, 8316, 517, 11, 314, 198, 69, 3883, 11, 14959, 13, 843, 287, 3357, 757, 11, 314, 12414, 13, 921, 750, 407, 1560, 502, 198, 5562, 345, 5292, 284, 467, 656, 19356, 13, 447, 251, 198, 198, 447, 250, 6423, 11, 703, 466, 345, 760, 30, 447, 251, 198, 198, 447, 250, 40, 766, 340, 11, 314, 4648, 7234, 340, 13, 1374, 466, 314, 760, 326, 345, 423, 587, 1972, 198, 14108, 944, 845, 9583, 16537, 11, 290, 326, 345, 423, 257, 749, 39120, 290, 36138, 198, 3168, 415, 2576, 30, 447, 251, 198, 198, 447, 250, 3666, 13674, 17628, 11, 447, 251, 531, 314, 11, 564, 250, 5661, 318, 1165, 881, 13, 921, 561, 3729, 423, 198, 47436, 11544, 11, 550, 345, 5615, 257, 1178, 10675, 2084, 13, 632, 318, 2081, 326, 314]\n",
      "y: [22474, 42934, 1156, 284, 465, 4692, 11, 7141, 475, 6178, 343, 1346, 12974, 2000, 13, 679, 198, 9776, 11, 314, 1011, 340, 11, 262, 749, 2818, 14607, 290, 21769, 4572, 326, 198, 1169, 995, 468, 1775, 11, 475, 355, 257, 18854, 339, 561, 423, 4624, 2241, 287, 257, 198, 9562, 2292, 13, 679, 1239, 5158, 286, 262, 32359, 30477, 11, 3613, 351, 257, 308, 32438, 198, 392, 257, 10505, 263, 13, 1119, 547, 37959, 1243, 329, 262, 22890, 960, 1069, 5666, 329, 198, 19334, 278, 262, 30615, 422, 1450, 447, 247, 82, 21508, 290, 4028, 13, 887, 329, 262, 8776, 198, 41181, 263, 284, 9159, 884, 9913, 15880, 656, 465, 898, 19217, 290, 32566, 198, 29117, 36140, 373, 284, 10400, 257, 36441, 5766, 543, 1244, 198, 16939, 257, 4719, 2402, 477, 465, 5110, 2482, 13, 402, 799, 287, 257, 8564, 198, 259, 43872, 11, 393, 257, 8469, 287, 530, 286, 465, 898, 1029, 12, 6477, 18405, 11, 561, 407, 198, 1350, 517, 14851, 621, 257, 1913, 9942, 287, 257, 3450, 884, 355, 465, 13, 843, 198, 25907, 612, 373, 475, 530, 2415, 284, 683, 11, 290, 326, 2415, 373, 262, 2739, 7181, 710, 198, 2782, 1754, 11, 286, 25292, 290, 18269, 4088, 13, 198, 198, 40, 550, 1775, 1310, 286, 17628, 16537, 13, 2011, 4845, 550, 38648, 514, 1497, 198, 6738, 1123, 584, 13, 2011, 898, 1844, 12157, 11, 290, 262, 1363, 12, 1087, 445, 198, 9446, 82, 543, 4485, 510, 1088, 262, 582, 508, 717, 7228, 2241, 4958, 198, 1659, 465, 898, 9323, 11, 547, 6751, 284, 17565, 477, 616, 3241, 11, 198, 4514, 17628, 11, 508, 2376, 35932, 790, 1296, 286, 3592, 351, 465, 2187, 45560, 666, 198, 82, 2852, 11, 6150, 287, 674, 19527, 70, 654, 287, 14372, 3530, 11, 11694, 1871, 465, 1468, 198, 12106, 11, 290, 39623, 422, 1285, 284, 1285, 1022, 15144, 290, 20505, 11, 198, 1169, 288, 8516, 1272, 286, 262, 2563, 11, 290, 262, 14800, 2568, 286, 465, 898, 13795, 198, 21353, 13, 679, 373, 991, 11, 355, 1683, 11, 7744, 12725, 416, 262, 2050, 286, 4065, 11, 198, 392, 12030, 465, 13964, 47126, 290, 11359, 5635, 286, 198, 672, 3168, 341, 287, 1708, 503, 883, 20195, 11, 290, 17304, 510, 883, 198, 1820, 1706, 444, 543, 550, 587, 9958, 355, 23292, 416, 262, 1743, 1644, 13, 198, 4863, 640, 284, 640, 314, 2982, 617, 13443, 1848, 286, 465, 466, 654, 25, 286, 465, 198, 16345, 11567, 284, 10529, 21411, 287, 262, 1339, 286, 262, 4700, 79, 2364, 5123, 11, 286, 465, 17304, 510, 198, 1659, 262, 18032, 13574, 286, 262, 49042, 9397, 379, 33822, 785, 1000, 68, 11, 290, 198, 69, 3289, 286, 262, 4365, 543, 339, 550, 13013, 523, 8675, 1286, 290, 198, 37351, 329, 262, 40098, 1641, 286, 16070, 13, 12197, 777, 5895, 286, 198, 14363, 3842, 11, 2158, 11, 543, 314, 6974, 4888, 351, 477, 262, 7183, 286, 198, 1169, 4445, 1803, 11, 314, 2993, 1310, 286, 616, 1966, 1545, 290, 15185, 13, 198, 198, 3198, 1755, 960, 270, 373, 319, 262, 29112, 286, 2805, 11, 49584, 960, 40, 373, 8024, 422, 257, 198, 73, 5604, 284, 257, 5827, 357, 1640, 314, 550, 783, 4504, 284, 3026, 3357, 828, 618, 198, 1820, 835, 2957, 502, 832, 14372, 3530, 13, 1081, 314, 3804, 262, 880, 12, 2787, 368, 9451, 198, 9424, 11, 543, 1276, 1464, 307, 3917, 287, 616, 2000, 351, 616, 36440, 278, 11, 290, 198, 4480, 262, 3223, 10207, 286, 262, 12481, 287, 26620, 11, 314, 373, 12000, 351, 257, 198, 365, 268, 6227, 284, 766, 17628, 757, 11, 290, 284, 760, 703, 339, 373, 26490, 465, 198, 26086, 35947, 5635, 13, 2399, 9519, 547, 37928, 6578, 11, 290, 11, 772, 355, 314, 198, 5460, 276, 510, 11, 314, 2497, 465, 7331, 11, 13952, 3785, 1208, 5403, 287, 257, 3223, 41834, 198, 32826, 262, 7770, 13, 679, 373, 37572, 262, 2119, 23994, 11, 30130, 11, 351, 465, 198, 2256, 24790, 2402, 465, 7721, 290, 465, 2832, 47425, 276, 2157, 683, 13, 1675, 502, 11, 508, 198, 74, 3605, 465, 790, 10038, 290, 7947, 11, 465, 9408, 290, 5642, 1297, 511, 898, 198, 13571, 13, 679, 373, 379, 670, 757, 13, 679, 550, 17450, 503, 286, 465, 2563, 12, 25598, 198, 25966, 82, 290, 373, 3024, 2402, 262, 21212, 286, 617, 649, 1917, 13, 314, 28077, 262, 8966, 198, 392, 373, 3402, 510, 284, 262, 11847, 543, 550, 15734, 587, 287, 636, 616, 898, 13, 198, 198, 6653, 5642, 373, 407, 914, 11350, 13, 632, 25129, 373, 26, 475, 339, 373, 9675, 11, 314, 892, 11, 198, 1462, 766, 502, 13, 2080, 8941, 257, 1573, 9635, 11, 475, 351, 257, 26820, 4151, 11, 339, 26834, 198, 1326, 284, 281, 3211, 16337, 11, 9617, 1973, 465, 1339, 286, 33204, 11, 290, 8203, 257, 198, 38685, 1339, 290, 257, 3623, 20878, 287, 262, 5228, 13, 3244, 339, 6204, 878, 262, 2046, 198, 392, 3114, 502, 625, 287, 465, 18032, 18951, 49540, 6977, 13, 198, 198, 447, 250, 19864, 5354, 14803, 345, 11, 447, 251, 339, 24998, 13, 564, 250, 40, 892, 11, 14959, 11, 326, 345, 423, 1234, 198, 261, 3598, 290, 257, 2063, 8059, 1201, 314, 2497, 345, 13, 447, 251, 198, 198, 447, 250, 31334, 0, 447, 251, 314, 9373, 13, 198, 198, 447, 250, 17854, 11, 314, 815, 423, 1807, 257, 1310, 517, 13, 2329, 257, 491, 8316, 517, 11, 314, 198, 69, 3883, 11, 14959, 13, 843, 287, 3357, 757, 11, 314, 12414, 13, 921, 750, 407, 1560, 502, 198, 5562, 345, 5292, 284, 467, 656, 19356, 13, 447, 251, 198, 198, 447, 250, 6423, 11, 703, 466, 345, 760, 30, 447, 251, 198, 198, 447, 250, 40, 766, 340, 11, 314, 4648, 7234, 340, 13, 1374, 466, 314, 760, 326, 345, 423, 587, 1972, 198, 14108, 944, 845, 9583, 16537, 11, 290, 326, 345, 423, 257, 749, 39120, 290, 36138, 198, 3168, 415, 2576, 30, 447, 251, 198, 198, 447, 250, 3666, 13674, 17628, 11, 447, 251, 531, 314, 11, 564, 250, 5661, 318, 1165, 881, 13, 921, 561, 3729, 423, 198, 47436, 11544, 11, 550, 345, 5615, 257, 1178, 10675, 2084, 13, 632, 318, 2081, 326, 314, 550]\n"
     ]
    }
   ],
   "source": [
    "enc_sample = encoded_text[500:]\n",
    "context_size = 1024\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ff63a",
   "metadata": {},
   "source": [
    "# Start predicting next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebc878f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [198] => Next token: 22474\n",
      "Context: [198, 22474] => Next token: 42934\n",
      "Context: [198, 22474, 42934] => Next token: 1156\n",
      "Context: [198, 22474, 42934, 1156] => Next token: 284\n",
      "Context: [198, 22474, 42934, 1156, 284] => Next token: 465\n",
      "Context: [198, 22474, 42934, 1156, 284, 465] => Next token: 4692\n",
      "Context: [198, 22474, 42934, 1156, 284, 465, 4692] => Next token: 11\n",
      "Context: [198, 22474, 42934, 1156, 284, 465, 4692, 11] => Next token: 7141\n",
      "Context: [198, 22474, 42934, 1156, 284, 465, 4692, 11, 7141] => Next token: 475\n",
      "Context: [198, 22474, 42934, 1156, 284, 465, 4692, 11, 7141, 475] => Next token: 6178\n",
      "Context: [198, 22474, 42934, 1156, 284, 465, 4692, 11, 7141, 475, 6178] => Next token: 343\n",
      "Context: [198, 22474, 42934, 1156, 284, 465, 4692, 11, 7141, 475, 6178, 343] => Next token: 1346\n",
      "Context: [198, 22474, 42934, 1156, 284, 465, 4692, 11, 7141, 475, 6178, 343, 1346] => Next token: 12974\n",
      "Context: [198, 22474, 42934, 1156, 284, 465, 4692, 11, 7141, 475, 6178, 343, 1346, 12974] => Next token: 2000\n",
      "Context: [198, 22474, 42934, 1156, 284, 465, 4692, 11, 7141, 475, 6178, 343, 1346, 12974, 2000] => Next token: 13\n",
      "Context: [198, 22474, 42934, 1156, 284, 465, 4692, 11, 7141, 475, 6178, 343, 1346, 12974, 2000, 13] => Next token: 679\n",
      "Context: [198, 22474, 42934, 1156, 284, 465, 4692, 11, 7141, 475, 6178, 343, 1346, 12974, 2000, 13, 679] => Next token: 198\n",
      "Context: [198, 22474, 42934, 1156, 284, 465, 4692, 11, 7141, 475, 6178, 343, 1346, 12974, 2000, 13, 679, 198] => Next token: 9776\n",
      "Context: [198, 22474, 42934, 1156, 284, 465, 4692, 11, 7141, 475, 6178, 343, 1346, 12974, 2000, 13, 679, 198, 9776] => Next token: 11\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,20):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(f\"Context: {context} => Next token: {desired}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f453c9f",
   "metadata": {},
   "source": [
    "# Convert to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d5e69c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 -> 22474 (\n",
      " -> were)\n",
      "22474 -> 42934 (were ->  abhor)\n",
      "42934 -> 1156 ( abhor -> rent)\n",
      "1156 -> 284 (rent ->  to)\n",
      "284 -> 465 ( to ->  his)\n",
      "465 -> 4692 ( his ->  cold)\n",
      "4692 -> 11 ( cold -> ,)\n",
      "11 -> 7141 (, ->  precise)\n",
      "7141 -> 475 ( precise ->  but)\n",
      "475 -> 6178 ( but ->  adm)\n",
      "6178 -> 343 ( adm -> ir)\n",
      "343 -> 1346 (ir -> ably)\n",
      "1346 -> 12974 (ably ->  balanced)\n",
      "12974 -> 2000 ( balanced ->  mind)\n",
      "2000 -> 13 ( mind -> .)\n",
      "13 -> 679 (. ->  He)\n",
      "679 -> 198 ( He -> \n",
      ")\n",
      "198 -> 9776 (\n",
      " -> was)\n",
      "9776 -> 11 (was -> ,)\n",
      "11 -> 314 (, ->  I)\n",
      "314 -> 1011 ( I ->  take)\n",
      "1011 -> 340 ( take ->  it)\n",
      "340 -> 11 ( it -> ,)\n",
      "11 -> 262 (, ->  the)\n",
      "262 -> 749 ( the ->  most)\n",
      "749 -> 2818 ( most ->  perfect)\n",
      "2818 -> 14607 ( perfect ->  reasoning)\n",
      "14607 -> 290 ( reasoning ->  and)\n",
      "290 -> 21769 ( and ->  observing)\n",
      "21769 -> 4572 ( observing ->  machine)\n",
      "4572 -> 326 ( machine ->  that)\n",
      "326 -> 198 ( that -> \n",
      ")\n",
      "198 -> 1169 (\n",
      " -> the)\n",
      "1169 -> 995 (the ->  world)\n",
      "995 -> 468 ( world ->  has)\n",
      "468 -> 1775 ( has ->  seen)\n",
      "1775 -> 11 ( seen -> ,)\n",
      "11 -> 475 (, ->  but)\n",
      "475 -> 355 ( but ->  as)\n",
      "355 -> 257 ( as ->  a)\n",
      "257 -> 18854 ( a ->  lover)\n",
      "18854 -> 339 ( lover ->  he)\n",
      "339 -> 561 ( he ->  would)\n",
      "561 -> 423 ( would ->  have)\n",
      "423 -> 4624 ( have ->  placed)\n",
      "4624 -> 2241 ( placed ->  himself)\n",
      "2241 -> 287 ( himself ->  in)\n",
      "287 -> 257 ( in ->  a)\n",
      "257 -> 198 ( a -> \n",
      ")\n",
      "198 -> 9562 (\n",
      " -> false)\n",
      "9562 -> 2292 (false ->  position)\n",
      "2292 -> 13 ( position -> .)\n",
      "13 -> 679 (. ->  He)\n",
      "679 -> 1239 ( He ->  never)\n",
      "1239 -> 5158 ( never ->  spoke)\n",
      "5158 -> 286 ( spoke ->  of)\n",
      "286 -> 262 ( of ->  the)\n",
      "262 -> 32359 ( the ->  softer)\n",
      "32359 -> 30477 ( softer ->  passions)\n",
      "30477 -> 11 ( passions -> ,)\n",
      "11 -> 3613 (, ->  save)\n",
      "3613 -> 351 ( save ->  with)\n",
      "351 -> 257 ( with ->  a)\n",
      "257 -> 308 ( a ->  g)\n",
      "308 -> 32438 ( g -> ibe)\n",
      "32438 -> 198 (ibe -> \n",
      ")\n",
      "198 -> 392 (\n",
      " -> and)\n",
      "392 -> 257 (and ->  a)\n",
      "257 -> 10505 ( a ->  sne)\n",
      "10505 -> 263 ( sne -> er)\n",
      "263 -> 13 (er -> .)\n",
      "13 -> 1119 (. ->  They)\n",
      "1119 -> 547 ( They ->  were)\n",
      "547 -> 37959 ( were ->  admirable)\n",
      "37959 -> 1243 ( admirable ->  things)\n",
      "1243 -> 329 ( things ->  for)\n",
      "329 -> 262 ( for ->  the)\n",
      "262 -> 22890 ( the ->  observer)\n",
      "22890 -> 960 ( observer -> â€”)\n",
      "960 -> 1069 (â€” -> ex)\n",
      "1069 -> 5666 (ex -> cellent)\n",
      "5666 -> 329 (cellent ->  for)\n",
      "329 -> 198 ( for -> \n",
      ")\n",
      "198 -> 19334 (\n",
      " -> draw)\n",
      "19334 -> 278 (draw -> ing)\n",
      "278 -> 262 (ing ->  the)\n",
      "262 -> 30615 ( the ->  veil)\n",
      "30615 -> 422 ( veil ->  from)\n",
      "422 -> 1450 ( from ->  men)\n",
      "1450 -> 447 ( men -> ï¿½)\n",
      "447 -> 247 (ï¿½ -> ï¿½)\n",
      "247 -> 82 (ï¿½ -> s)\n",
      "82 -> 21508 (s ->  motives)\n",
      "21508 -> 290 ( motives ->  and)\n",
      "290 -> 4028 ( and ->  actions)\n",
      "4028 -> 13 ( actions -> .)\n",
      "13 -> 887 (. ->  But)\n",
      "887 -> 329 ( But ->  for)\n",
      "329 -> 262 ( for ->  the)\n",
      "262 -> 8776 ( the ->  trained)\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(f\"{x[i]} -> {y[i]} ({tokenizer.decode([x[i]])} -> {tokenizer.decode([y[i]])})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sujar-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
